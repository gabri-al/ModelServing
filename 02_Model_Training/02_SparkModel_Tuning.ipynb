{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f83b3f4-16e0-4419-8dd9-4aa8611bcaf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# AirBnb NY Listing Price Prediction: Spark Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2faee6f-2d67-40db-bbcb-81df4d626e64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating distributed RandomForest model tuning from the `spark.ml` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f1eeae-0882-4279-a63f-3988aa6103fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ac092d-8a84-4799-a566-b1d6c57a0126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Define Global Variables\n",
    "catalog_ = os.getenv('CATALOG_NAME')\n",
    "schema_ = os.getenv('SCHEMA_NAME')\n",
    "spark.sql(\"USE CATALOG \"+catalog_)\n",
    "spark.sql(\"USE SCHEMA \"+schema_)\n",
    "\n",
    "SEED_ = 111\n",
    "Target_Var_ = 'price_log'\n",
    "experiment_name_ = 'Airbnb_NY_Tuning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6b27fb-0722-42bf-8e11-3e0a348c717a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read Gold Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d87aae-c07a-4c5f-a974-95ef39e5f345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import gold data\n",
    "gold_data = spark.sql(\"SELECT * from airbnb_ny_gold_data\")\n",
    "display(gold_data.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde01658-b202-4e2b-bfc4-36d6b913fbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Double-check Presence of NAs values\n",
    "# display(dbutils.data.summarize(gold_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40bc273-3466-46cd-b078-9881e0e57528",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Double-check DataTypes - pyspark.ml.regression.RandomForestRegressor supports both Float and Double\n",
    "for field in gold_data.schema.fields:\n",
    "    print(f\"Column '{field.name}' has data type: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f010c6e0-a893-4888-a717-c9bb64af2055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split into Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21415abb-4e59-497c-8b5a-cced753d1ca1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Split\n",
    "train_df, test_df = gold_data.randomSplit([.85, .15], seed = SEED_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea033af1-54d9-4005-916a-6e2915b84181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configure Hyperparameter Tuning with RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c018272-c64a-403a-9737-663458cd52d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create Features\n",
    "feature_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"float\") & (field != \"price\") & (field != \"id\") & (field != \"price_log\"))]\n",
    "vec_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "### Create Base Model\n",
    "RF_ = RandomForestRegressor(labelCol = Target_Var_, seed = SEED_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576df98b-4ce6-4097-9a7e-8eb4c8a211a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create the Parameter Space\n",
    "paramGrid_ = (ParamGridBuilder()\n",
    "\t\t.addGrid(RF_.maxDepth, [2, 5, 10, 20]) # max 30\n",
    "\t\t.addGrid(RF_.numTrees, [10, 30, 100, 200, 300])\n",
    "    .addGrid(RF_.featureSubsetStrategy, ['onethird', 'sqrt', 'log2'])\n",
    "\t\t.build())\n",
    "\n",
    "### Create the Evaluator\n",
    "evaluator_ = RegressionEvaluator(\n",
    "    labelCol = Target_Var_,\n",
    "\t\tpredictionCol = \"prediction\", # predicted price, as calculated by the model\n",
    "\t\tmetricName = \"r2\")\n",
    "\n",
    "### Create the CV object\n",
    "cv_ = CrossValidator(estimator = RF_,\n",
    "\t\t\testimatorParamMaps = paramGrid_,\n",
    "\t\t\tevaluator = evaluator_,\n",
    "\t\t\tnumFolds = 3,\n",
    "\t\t\tparallelism = 10, # nr of models to train in parallel, do not put all available cores, but set it up to 10 (https://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "\t\t\tseed = SEED_)\n",
    "\n",
    "#### Create the Pipeline\n",
    "Pipeline_ = Pipeline(stages = [vec_assembler, cv_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b992cd4-c715-4282-bf18-94ea7bedf48e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Launch the pipeline in an mlflow run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9283be38-4bdc-49a8-acf7-c15f56fe348e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Set up the MLFlow Experiment\n",
    "experiment_path = f'/Users/gabriele.albini@databricks.com/{experiment_name_}'\n",
    "experiment = mlflow.get_experiment_by_name(experiment_path)\n",
    "\n",
    "if experiment is not None:\n",
    "    experiment_id = experiment.experiment_id\n",
    "else:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_path)\n",
    "\n",
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac948979-cd7a-4360-86e0-4721ed56d83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Launch the run\n",
    "mlflow.autolog(disable=True)\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"RF_Spark_cv\"):\n",
    "\n",
    "    # Fit the Pipeline object\n",
    "    Pipeline_fitted = Pipeline_.fit(train_df)\n",
    "\n",
    "    # Extract cv step from pipeline\n",
    "    cv_fitted = Pipeline_fitted.stages[-1]\n",
    "\n",
    "    # Start nested runs to ;og params and metric for each model\n",
    "    for i in range(len(cv_fitted.getEstimatorParamMaps())):\n",
    "\n",
    "      with mlflow.start_run(experiment_id=experiment_id, run_name=\"RF_model_\"+str(i+1), nested=True):\n",
    "\n",
    "        ## Log Params\n",
    "        params_ = cv_fitted.getEstimatorParamMaps()[i]\n",
    "        for p in params_:\n",
    "          p_pretty = str(p).split('__') # Extract parameter name from a string like 'RandomForestRegressor_ef34cc9d8f09__maxDepth'\n",
    "          mlflow.log_param(str(p_pretty[-1]), params_[p])\n",
    "\n",
    "        ## Log Metric as avg across folds\n",
    "        avg_metric = cv_fitted.avgMetrics[i]\n",
    "        mlflow.log_metric(\"Obj Metric\", avg_metric)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4145428257173881,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_SparkModel_Tuning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
